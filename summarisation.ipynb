{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3250ec",
   "metadata": {},
   "source": [
    "# üìò Summarization Model with LoRA and PEFT using FLAN-T5\n",
    "\n",
    "### üîç Objective:\n",
    "The goal of this notebook is to build a lightweight, instruction-following **summarization model** that generates concise answers from a given **question + context** input.\n",
    "\n",
    "We fine-tune the **FLAN-T5** model using a **parameter-efficient method** (PEFT) called **LoRA (Low-Rank Adaptation)**. The dataset consists of context-question-answer triples, where the answer serves as a summary of the context in response to the question.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Approach Summary:\n",
    "- **Model**: `google/flan-t5-base` (pretrained on instruction-following tasks)\n",
    "- **Task framing**: Instruction-based summarization using `\"summarize: \"` prefix\n",
    "- **Data**: `neural-bridge/rag-dataset-12000` (QA-style summarization)\n",
    "- **Fine-tuning**: We apply **LoRA adapters** to inject trainable parameters without updating the full model\n",
    "- **Training Framework**: `adapter-transformers` + HuggingFace Trainer\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a technique for fine-tuning large language models by **inserting trainable rank-decomposed matrices** into each layer, while **keeping the original weights frozen**.\n",
    "\n",
    "Instead of updating the full weight matrix $W \\in \\mathbb{R}^{d \\times d}$ , LoRA approximates the update as:\n",
    "\n",
    "$$\n",
    "\\Delta W \\approx A B \\quad \\text{where } A \\in \\mathbb{R}^{d \\times r}, \\; B \\in \\mathbb{R}^{r \\times d}, \\; r \\ll d\n",
    "$$\n",
    "\n",
    "This drastically reduces the number of trainable parameters and allows efficient adaptation with less compute and memory.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© What is PEFT?\n",
    "\n",
    "**PEFT (Parameter-Efficient Fine-Tuning)** refers to any technique that fine-tunes only a **subset of a model's parameters**.  \n",
    "LoRA is one such method under this umbrella, making it possible to:\n",
    "- Reuse the same base model across tasks\n",
    "- Add/remove adapters without retraining\n",
    "- Reduce storage and deployment cost\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Why This Setup?\n",
    "\n",
    "Fine-tuning large models like FLAN-T5 from scratch is expensive and often unnecessary.  \n",
    "LoRA + PEFT lets us train compact and effective models even on modest hardware, making this setup ideal for:\n",
    "- Domain adaptation\n",
    "- Instruction tuning\n",
    "- Fast experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083959f",
   "metadata": {},
   "source": [
    "## 1. Load Tokenizer\n",
    "\n",
    "We load the tokenizer for the FLAN-T5 base model.  \n",
    "This tokenizer will be used for encoding inputs (question + context) and decoding model outputs (answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30269db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "prefix = 'summarize: '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8d0f9",
   "metadata": {},
   "source": [
    "## 2. Define Tokenization Function\n",
    "\n",
    "This function encodes a batch of data:\n",
    "- Inputs are created by concatenating `question` + `context`, prefixed with `\"summarize: \"`.\n",
    "- Targets are the `answer` texts.\n",
    "- Both are tokenized with truncation and padding.\n",
    "- The result is returned as a dictionary with `input_ids` and `labels`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(examples):\n",
    "    text_column1 = 'context'\n",
    "    text_column2 = 'question'\n",
    "    summary_column = 'answer'\n",
    "    \n",
    "    padding = \"max_length\"\n",
    "\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(examples[text_column1])):\n",
    "        if examples[text_column1][i] and examples[text_column2][i] and examples[summary_column][i]:\n",
    "            # Concatenate question + context\n",
    "            input_text = examples[text_column2][i] + \" \" + examples[text_column1][i]\n",
    "            inputs.append(input_text)\n",
    "            targets.append(examples[summary_column][i])\n",
    "\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding=padding, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding=padding, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b018e",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "This function loads the dataset split (`train` or `test`) and:\n",
    "- Filters out rows with missing `context` or `answer`\n",
    "- Limits the number of rows to `max_items`\n",
    "- Applies tokenization using `encode_batch`\n",
    "- Formats the dataset as PyTorch tensors for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffec54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_name, max_items):\n",
    "    \n",
    "    dataset = load_dataset(\"neural-bridge/rag-dataset-12000\")[split_name] \n",
    "\n",
    "\n",
    "    dataset = dataset.filter(lambda example: example['context'] is not None and example['answer'] is not None)\n",
    "    \n",
    "    dataset = dataset.filter(lambda _, idx: idx < max_items, with_indices=True)\n",
    "    \n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        encode_batch,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Running tokenizer on \" + split_name + \" dataset\",\n",
    "    )\n",
    "    \n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d0b65",
   "metadata": {},
   "source": [
    "## 4. Load Model and Add LoRA Adapter\n",
    "\n",
    "We use the AdapterHub-compatible model (`AutoAdapterModel`) and apply **LoRA (Low-Rank Adaptation)**:\n",
    "- `r = 8`: Low-rank dimensionality\n",
    "- `alpha = 16`: Scaling factor\n",
    "- `intermediate_lora` and `output_lora`: Apply LoRA to both FFN and output layers\n",
    "\n",
    "This enables **parameter-efficient fine-tuning** without updating the entire base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ee3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoModelForSeq2SeqLM\n",
    "from adapters import LoRAConfig\n",
    "\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(base_model)\n",
    "\n",
    "# Load the model\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
    "\n",
    "config = LoRAConfig(\n",
    "    r=8,\n",
    "    alpha=16,\n",
    "    intermediate_lora=True,\n",
    "    output_lora=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "044b9936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.t5.modeling_t5.T5ForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b49ef99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'adapters.models.t5.adapter_model.T5AdapterModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3de852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add_adapter(\"my_summary_adapter\", config=config, adapter_type=\"lora\")\n",
    "model.add_adapter(adapter_name=\"my_summary_adapter\", config=config)\n",
    "\n",
    "model.train_adapter(\"my_summary_adapter\")\n",
    "model.set_active_adapters(\"my_summary_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b626fdd",
   "metadata": {},
   "source": [
    "## 5. Define Training Configuration and Start Training\n",
    "\n",
    "We configure the trainer using HuggingFace's `TrainingArguments`:\n",
    "- 2 epochs\n",
    "- Batch size of 2\n",
    "- Logging every 50 steps\n",
    "\n",
    "We use `AdapterTrainer` to train only the adapter layer, leaving the base model frozen.\n",
    "\n",
    "Training and evaluation are performed on subsets of 1,000 training and 100 test examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57144e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e47ab3f5a04eb98ae60343839c9b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3549dff46e4ce6b781abd9f8e3efe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc7cebef7b244678eba63f3a0d75fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e10caf5fba849beaf0a99f5d507b267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623d2d0a50d7458b923d4fe74d3ed61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89faa17c7b2e4183bb56cb43efbcf4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on test dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>23.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.827300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.558600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.435400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.535200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.422000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=2.032686315536499, metrics={'train_runtime': 383.374, 'train_samples_per_second': 5.217, 'train_steps_per_second': 2.608, 'total_flos': 1381594300416000.0, 'train_loss': 2.032686315536499, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from adapters import AdapterTrainer\n",
    "from datasets import load_dataset\n",
    "batch_size = 2  \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_steps=50,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=load_split(\"train\", 1000),\n",
    "    eval_dataset=load_split(\"test\", 100),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4e2302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3849603533744812,\n",
       " 'eval_runtime': 6.2349,\n",
       " 'eval_samples_per_second': 16.039,\n",
       " 'eval_steps_per_second': 8.019,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31911ef8",
   "metadata": {},
   "source": [
    "## 6. Merge Adapter with Base Model\n",
    "\n",
    "After training, the adapter is merged into the base model so it can be used for standalone inference without adapter activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec80e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.merge_adapter(\"my_summary_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368aaaf4",
   "metadata": {},
   "source": [
    "## 7. Run Inference\n",
    "\n",
    "We test the trained summarization model by passing in a question and a long context.  \n",
    "The model generates a summary using its learned instruction-following ability.\n",
    "\n",
    "#### Output:\n",
    "> **Generated Summary**:  \n",
    "> *The story of the magic mill spread far and wide.*\n",
    "\n",
    "This shows the model's ability to extract and compress key information into a brief, high-level summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ea6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " summarize: Summarize the story. \n",
      "Once upon a time, there were two brothers ‚Äî one was rich, and the other was poor. The poor brother ran out of food and went to his rich brother, begging for something to eat.\n",
      "\n",
      "The rich brother, not happy about helping, said, ‚ÄúI‚Äôll give you this ham, but you must take it to Dead Man‚Äôs Hall.‚Äù\n",
      "\n",
      "Grateful for the food, the poor brother agreed. He walked all day and finally reached a large building at dusk. Outside, an old man was chopping wood.\n",
      "\n",
      "‚ÄúExcuse me, sir,‚Äù said the poor brother. ‚ÄúIs this the way to Dead Man‚Äôs Hall?‚Äù\n",
      "\n",
      "‚ÄúYes, you‚Äôve arrived,‚Äù replied the old man. ‚ÄúInside, they will want to buy your ham. But don‚Äôt sell it unless they give you the hand-mill that stands behind the door.‚Äù\n",
      "\n",
      "The poor brother thanked the old man, went inside, and everything happened just as the old man had said. The poor brother left with the hand-mill and asked the old man how to use it. Then, he set off home.\n",
      "\n",
      "The hand-mill was magical. When the poor brother got home, he asked it to grind a feast of food and drink. To stop the mill, he simply had to say, ‚ÄúThank you, magic mill, you can stop now.‚Äù\n",
      "\n",
      "When the rich brother saw that his brother was no longer poor, he became jealous. ‚ÄúGive me that mill!‚Äù he demanded. The poor brother, having everything he needed, agreed to sell it but didn‚Äôt tell his rich brother how to stop it.\n",
      "\n",
      "The rich brother eagerly asked the mill to grind food when he got home, but because he didn‚Äôt know how to stop it, the mill kept grinding until food overflowed from the house and across the fields. In a panic, he ran to his poor brother‚Äôs house. ‚ÄúPlease take it back!‚Äù he cried. ‚ÄúIf it doesn‚Äôt stop, the whole town will be buried!‚Äù\n",
      "\n",
      "The poor brother took the mill back and was never poor or hungry again.\n",
      "\n",
      "Soon, the story of the magic mill spread far and wide. One day, a sailor knocked at the poor brother‚Äôs door. ‚ÄúDoes the mill grind salt?‚Äù he asked.\n",
      "\n",
      "‚ÄúOf course,‚Äù replied the brother. ‚ÄúIt will grind anything you ask.‚Äù\n",
      "\n",
      "The sailor, eager to stop traveling far for salt, offered a thousand coins for the mill. Though the brother was hesitant, he eventually agreed.\n",
      "\n",
      "In his hurry, the sailor forgot to ask how to stop the mill. Once at sea, he placed the mill on deck and commanded, ‚ÄúGrind salt, and grind quickly!‚Äù\n",
      "\n",
      "The mill obeyed, but it didn‚Äôt stop. The pile of salt grew and grew until the ship sank under its weight.\n",
      "\n",
      "The mill still lies at the bottom of the sea, grinding salt to this day, and that‚Äôs why the sea is salty.\n",
      "\n",
      "\n",
      "\n",
      "Generated Summary:\n",
      " The story of the magic mill spread far and wide.\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "Once upon a time, there were two brothers ‚Äî one was rich, and the other was poor. The poor brother ran out of food and went to his rich brother, begging for something to eat.\n",
    "\n",
    "The rich brother, not happy about helping, said, ‚ÄúI‚Äôll give you this ham, but you must take it to Dead Man‚Äôs Hall.‚Äù\n",
    "\n",
    "Grateful for the food, the poor brother agreed. He walked all day and finally reached a large building at dusk. Outside, an old man was chopping wood.\n",
    "\n",
    "‚ÄúExcuse me, sir,‚Äù said the poor brother. ‚ÄúIs this the way to Dead Man‚Äôs Hall?‚Äù\n",
    "\n",
    "‚ÄúYes, you‚Äôve arrived,‚Äù replied the old man. ‚ÄúInside, they will want to buy your ham. But don‚Äôt sell it unless they give you the hand-mill that stands behind the door.‚Äù\n",
    "\n",
    "The poor brother thanked the old man, went inside, and everything happened just as the old man had said. The poor brother left with the hand-mill and asked the old man how to use it. Then, he set off home.\n",
    "\n",
    "The hand-mill was magical. When the poor brother got home, he asked it to grind a feast of food and drink. To stop the mill, he simply had to say, ‚ÄúThank you, magic mill, you can stop now.‚Äù\n",
    "\n",
    "When the rich brother saw that his brother was no longer poor, he became jealous. ‚ÄúGive me that mill!‚Äù he demanded. The poor brother, having everything he needed, agreed to sell it but didn‚Äôt tell his rich brother how to stop it.\n",
    "\n",
    "The rich brother eagerly asked the mill to grind food when he got home, but because he didn‚Äôt know how to stop it, the mill kept grinding until food overflowed from the house and across the fields. In a panic, he ran to his poor brother‚Äôs house. ‚ÄúPlease take it back!‚Äù he cried. ‚ÄúIf it doesn‚Äôt stop, the whole town will be buried!‚Äù\n",
    "\n",
    "The poor brother took the mill back and was never poor or hungry again.\n",
    "\n",
    "Soon, the story of the magic mill spread far and wide. One day, a sailor knocked at the poor brother‚Äôs door. ‚ÄúDoes the mill grind salt?‚Äù he asked.\n",
    "\n",
    "‚ÄúOf course,‚Äù replied the brother. ‚ÄúIt will grind anything you ask.‚Äù\n",
    "\n",
    "The sailor, eager to stop traveling far for salt, offered a thousand coins for the mill. Though the brother was hesitant, he eventually agreed.\n",
    "\n",
    "In his hurry, the sailor forgot to ask how to stop the mill. Once at sea, he placed the mill on deck and commanded, ‚ÄúGrind salt, and grind quickly!‚Äù\n",
    "\n",
    "The mill obeyed, but it didn‚Äôt stop. The pile of salt grew and grew until the ship sank under its weight.\n",
    "\n",
    "The mill still lies at the bottom of the sea, grinding salt to this day, and that‚Äôs why the sea is salty.\n",
    "\n",
    "\"\"\"\n",
    "question = \"Summarize the story.\"\n",
    "\n",
    "input_text = prefix + question + \" \" + context\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_length=128)\n",
    "\n",
    "generated_summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Input:\\n\", input_text)\n",
    "print(\"\\nGenerated Summary:\\n\", generated_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
